{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "\n",
    "#from user_definition import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def toDoubleSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except:\n",
    "        return str(v) #if it is not a float type return as a string.\n",
    "\n",
    "# def strip_time(x):\n",
    "#     x = x.strip(\"\\\"\")\n",
    "#     try:\n",
    "        \n",
    "#         return datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n",
    "#     except:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=kmeans#pyspark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "dfpen = ss.read.csv(\"../Data/penbased.dat\", samplingRatio=0.3, schema=penschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| pix1| pix2|pix3| pix4| pix5| pix6| pix7| pix8| pix9|pix10|pix11|pix12|pix13|pix14|pix15|pix16|label|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 47.0|100.0|27.0| 81.0| 57.0| 37.0| 26.0|  0.0|  0.0| 23.0| 56.0| 53.0|100.0| 90.0| 40.0| 98.0|  8.0|\n",
      "|  0.0| 89.0|27.0|100.0| 42.0| 75.0| 29.0| 45.0| 15.0| 15.0| 37.0|  0.0| 69.0|  2.0|100.0|  6.0|  2.0|\n",
      "|  0.0| 57.0|31.0| 68.0| 72.0| 90.0|100.0|100.0| 76.0| 75.0| 50.0| 51.0| 28.0| 25.0| 16.0|  0.0|  1.0|\n",
      "|  0.0|100.0| 7.0| 92.0|  5.0| 68.0| 19.0| 45.0| 86.0| 34.0|100.0| 45.0| 74.0| 23.0| 67.0|  0.0|  4.0|\n",
      "|  0.0| 67.0|49.0| 83.0|100.0|100.0| 81.0| 80.0| 60.0| 60.0| 40.0| 40.0| 33.0| 20.0| 47.0|  0.0|  1.0|\n",
      "|100.0|100.0|88.0| 99.0| 49.0| 74.0| 17.0| 47.0|  0.0| 16.0| 37.0|  0.0| 73.0| 16.0| 20.0| 20.0|  6.0|\n",
      "|  0.0|100.0| 3.0| 72.0| 26.0| 35.0| 85.0| 35.0|100.0| 71.0| 73.0| 97.0| 65.0| 49.0| 66.0|  0.0|  4.0|\n",
      "|  0.0| 39.0| 2.0| 62.0| 11.0|  5.0| 63.0|  0.0|100.0| 43.0| 89.0| 99.0| 36.0|100.0|  0.0| 57.0|  0.0|\n",
      "| 13.0| 89.0|12.0| 50.0| 72.0| 38.0| 56.0|  0.0|  4.0| 17.0|  0.0| 61.0| 32.0| 94.0|100.0|100.0|  5.0|\n",
      "| 74.0| 87.0|31.0|100.0|  0.0| 69.0| 62.0| 64.0|100.0| 79.0|100.0| 38.0| 84.0|  0.0| 18.0|  1.0|  9.0|\n",
      "| 48.0| 96.0|62.0| 65.0| 88.0| 27.0| 21.0|  0.0| 21.0| 33.0| 79.0| 67.0|100.0|100.0|  0.0| 85.0|  8.0|\n",
      "|100.0|100.0|72.0| 99.0| 36.0| 78.0| 34.0| 54.0| 79.0| 47.0| 64.0| 13.0| 19.0|  0.0|  0.0|  2.0|  5.0|\n",
      "| 91.0| 74.0|54.0|100.0|  0.0| 87.0| 23.0| 59.0| 81.0| 67.0|100.0| 39.0| 79.0|  4.0| 21.0|  0.0|  9.0|\n",
      "|  0.0| 85.0|38.0|100.0| 81.0| 88.0| 87.0| 50.0| 84.0| 12.0| 58.0|  0.0| 53.0| 22.0|100.0| 24.0|  7.0|\n",
      "| 35.0| 76.0|57.0|100.0|100.0| 92.0| 68.0| 66.0| 81.0| 38.0| 82.0|  9.0| 32.0|  0.0|  0.0| 17.0|  3.0|\n",
      "| 50.0| 84.0|66.0|100.0| 75.0| 75.0| 51.0| 51.0|100.0| 42.0| 97.0| 13.0| 49.0|  0.0|  0.0|  7.0|  3.0|\n",
      "| 99.0| 80.0|63.0|100.0| 25.0| 76.0| 79.0| 68.0|100.0| 62.0| 97.0| 23.0| 54.0|  0.0|  0.0| 16.0|  9.0|\n",
      "| 24.0| 66.0|43.0|100.0| 59.0| 65.0| 34.0| 28.0|  0.0|  1.0| 16.0| 11.0| 58.0|  0.0|100.0|  1.0|  2.0|\n",
      "|  0.0| 73.0|19.0| 99.0| 72.0|100.0| 70.0| 73.0| 32.0| 48.0|  5.0| 18.0| 46.0|  0.0|100.0|  0.0|  2.0|\n",
      "| 12.0| 77.0|20.0| 62.0| 78.0| 40.0| 50.0|  0.0|  1.0| 17.0|  0.0| 64.0| 23.0| 98.0|100.0|100.0|  5.0|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfpen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with a feature vector (Exclude the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vector_Assembler(df,y_column):\n",
    "    columns = df.columns\n",
    "    # remove y column\n",
    "    columns.remove(y_column)\n",
    "    va= VectorAssembler(inputCols=columns,outputCol='features').transform(df)\n",
    "    lpoints = va.select(\"features\", y_column).withColumnRenamed(y_column, \"label\")\n",
    "    return lpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[47.0,100.0,27.0,...|  8.0|\n",
      "|[0.0,89.0,27.0,10...|  2.0|\n",
      "|[0.0,57.0,31.0,68...|  1.0|\n",
      "|[0.0,100.0,7.0,92...|  4.0|\n",
      "|[0.0,67.0,49.0,83...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints = Vector_Assembler(dfpen,'label')\n",
    "lpoints.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "def normalize(train_df):\n",
    "    scaler = StandardScaler(inputCol='features',outputCol='scaled').fit(train_df)\n",
    "    scaled_df = scaler.transform(train_df)\\\n",
    "                        .drop('features')\\\n",
    "                        .withColumnRenamed('scaled','features')\\\n",
    "                        .select('features','label')\n",
    "    return scaler,scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, scaled_df = normalize(lpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.36818014358814...|  8.0|\n",
      "|[0.0,5.5001493387...|  2.0|\n",
      "|[0.0,3.5225675540...|  1.0|\n",
      "|[0.0,6.1799430772...|  4.0|\n",
      "|[0.0,4.1405618617...|  1.0|\n",
      "|[2.91102158210244...|  6.0|\n",
      "|[0.0,6.1799430772...|  4.0|\n",
      "|[0.0,2.4101778001...|  0.0|\n",
      "|[0.37843280567331...|  5.0|\n",
      "|[2.15415597075580...|  9.0|\n",
      "|[1.39729035940917...|  8.0|\n",
      "|[2.91102158210244...|  5.0|\n",
      "|[2.64902963971322...|  9.0|\n",
      "|[0.0,5.2529516156...|  7.0|\n",
      "|[1.01885755373585...|  3.0|\n",
      "|[1.45551079105122...|  3.0|\n",
      "|[2.88191136628141...|  9.0|\n",
      "|[0.69864517970458...|  2.0|\n",
      "|[0.0,4.5113584463...|  2.0|\n",
      "|[0.34932258985229...|  5.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# penlpoints= VectorAssembler(outputCol='features',inputCols=dfpen.columns[:-1]).transform(dfpen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply KMeans algorithm to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "model = KMeans(k = 10,maxIter=200,tol =0.1)\n",
    "kmodel = model.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[1.36818014358814...|  8.0|         5|\n",
      "|[0.0,5.5001493387...|  2.0|         0|\n",
      "|[0.0,3.5225675540...|  1.0|         3|\n",
      "|[0.0,6.1799430772...|  4.0|         2|\n",
      "|[0.0,4.1405618617...|  1.0|         3|\n",
      "|[2.91102158210244...|  6.0|         9|\n",
      "|[0.0,6.1799430772...|  4.0|         2|\n",
      "|[0.0,2.4101778001...|  0.0|         1|\n",
      "|[0.37843280567331...|  5.0|         8|\n",
      "|[2.15415597075580...|  9.0|         7|\n",
      "|[1.39729035940917...|  8.0|         5|\n",
      "|[2.91102158210244...|  5.0|         7|\n",
      "|[2.64902963971322...|  9.0|         7|\n",
      "|[0.0,5.2529516156...|  7.0|         0|\n",
      "|[1.01885755373585...|  3.0|         6|\n",
      "|[1.45551079105122...|  3.0|         6|\n",
      "|[2.88191136628141...|  9.0|         7|\n",
      "|[0.69864517970458...|  2.0|         0|\n",
      "|[0.0,4.5113584463...|  2.0|         0|\n",
      "|[0.34932258985229...|  5.0|         8|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = kmodel.transform(scaled_df)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/03 14:03:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmodel = KMeans(k = 10,maxIter=200,tol =0.1).fit(penlpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = kmodel.transform(penlpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.38474181, 4.99547303, 1.76607907, 5.1194924 , 2.10075496,\n",
       "        3.00980384, 1.8530205 , 1.52383537, 0.98279362, 0.51924834,\n",
       "        0.42525131, 0.36022228, 1.97317494, 0.50155665, 2.30023528,\n",
       "        0.51445069]),\n",
       " array([0.65639928, 4.85396223, 0.31194912, 2.2450629 , 0.77053946,\n",
       "        0.21634928, 2.28969031, 0.31437175, 2.82425296, 1.70642175,\n",
       "        2.2634699 , 3.18142842, 1.85456076, 2.84968096, 0.228292  ,\n",
       "        1.80972175]),\n",
       " array([1.23896964e+00, 6.10340639e+00, 7.21006247e-01, 4.14915227e+00,\n",
       "        1.12363019e-01, 1.92312032e+00, 1.53338797e+00, 1.44627262e+00,\n",
       "        2.57507269e+00, 1.95451742e+00, 2.31805660e+00, 2.23981260e+00,\n",
       "        3.19998655e+00, 9.45637265e-01, 1.49741955e+00, 2.93465250e-03]),\n",
       " array([0.1576717 , 3.58883883, 1.20773608, 3.68161291, 2.11677645,\n",
       "        3.32845759, 2.84915717, 3.1863166 , 2.26307428, 2.72883316,\n",
       "        1.91572589, 1.87351391, 2.5478725 , 0.72729471, 1.18475305,\n",
       "        0.03736315]),\n",
       " array([2.36903138, 5.14655466, 1.38936767, 5.10902105, 0.66640823,\n",
       "        2.49750902, 2.14033737, 1.05799787, 1.524394  , 0.02640886,\n",
       "        0.19712218, 0.74887414, 2.10733338, 1.67793804, 2.08736176,\n",
       "        2.34645981]),\n",
       " array([1.30582185, 5.64393704, 0.94238035, 3.65871284, 0.97294877,\n",
       "        1.10090118, 1.10017427, 0.04633119, 1.37808499, 0.70685178,\n",
       "        2.20988334, 2.05090691, 3.89165262, 2.74909225, 0.7173845 ,\n",
       "        2.53509231]),\n",
       " array([0.69211844, 5.21203648, 2.21906011, 5.15843764, 2.56542338,\n",
       "        3.18505544, 2.12507802, 2.04438304, 2.22559522, 1.48951982,\n",
       "        2.33736438, 0.67199733, 2.20722244, 0.19287421, 0.14154244,\n",
       "        0.28426805]),\n",
       " array([2.21408733, 5.40721669, 2.46991224, 4.74969193, 1.33371081,\n",
       "        3.10191235, 1.54137126, 2.52605505, 2.11688669, 2.32697082,\n",
       "        2.20723067, 1.29606979, 2.33494169, 0.27691713, 0.16707208,\n",
       "        0.11310515]),\n",
       " array([0.75686561, 5.55028035, 0.67931657, 3.15218027, 1.78410324,\n",
       "        1.38744988, 1.40853893, 0.09857943, 0.19532124, 0.67368304,\n",
       "        0.21701841, 2.36952987, 1.75442086, 2.83220252, 2.35114143,\n",
       "        2.7408354 ]),\n",
       " array([2.50690173, 6.05166842, 1.97123219, 4.5439474 , 0.63814217,\n",
       "        2.22154464, 0.21261456, 0.95489904, 0.96281132, 0.19351717,\n",
       "        2.16706244, 0.47148667, 2.89645951, 0.94488247, 0.41112037,\n",
       "        0.6889621 ])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers = kmodel.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.4307650706198399\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette= evaluator.evaluate(pred)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4404754324875236"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "ClusteringEvaluator().evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
