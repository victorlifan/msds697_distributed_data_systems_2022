{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "\n",
    "#from user_definition import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def toDoubleSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except:\n",
    "        return str(v) #if it is not a float type return as a string.\n",
    "\n",
    "# def strip_time(x):\n",
    "#     x = x.strip(\"\\\"\")\n",
    "#     try:\n",
    "        \n",
    "#         return datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n",
    "#     except:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('47.0, 100.0, 27.0, 81.0, 57.0, 37.0, 26.0, 0.0, 0.0, 23.0, 56.0, 53.0, 100.0, 90.0, 40.0, 98.0, 8.0',)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen_raw = sc.textFile(\"../Data/penbased.dat\")\n",
    "pen_raw.first(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd= pen_raw.map(lambda x: x.split(','))\\\n",
    "              .map(lambda x: [toDoubleSafe(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.0,\n",
       " 100.0,\n",
       " 27.0,\n",
       " 81.0,\n",
       " 57.0,\n",
       " 37.0,\n",
       " 26.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 23.0,\n",
       " 56.0,\n",
       " 53.0,\n",
       " 100.0,\n",
       " 90.0,\n",
       " 40.0,\n",
       " 98.0,\n",
       " 8.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|pix1| pix2|pix3| pix4| pix5| pix6| pix7| pix8|pix9|pix10|pix11|pix12|pix13|pix14|pix15|pix16|label|\n",
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|47.0|100.0|27.0| 81.0| 57.0| 37.0| 26.0|  0.0| 0.0| 23.0| 56.0| 53.0|100.0| 90.0| 40.0| 98.0|  8.0|\n",
      "| 0.0| 89.0|27.0|100.0| 42.0| 75.0| 29.0| 45.0|15.0| 15.0| 37.0|  0.0| 69.0|  2.0|100.0|  6.0|  2.0|\n",
      "| 0.0| 57.0|31.0| 68.0| 72.0| 90.0|100.0|100.0|76.0| 75.0| 50.0| 51.0| 28.0| 25.0| 16.0|  0.0|  1.0|\n",
      "| 0.0|100.0| 7.0| 92.0|  5.0| 68.0| 19.0| 45.0|86.0| 34.0|100.0| 45.0| 74.0| 23.0| 67.0|  0.0|  4.0|\n",
      "| 0.0| 67.0|49.0| 83.0|100.0|100.0| 81.0| 80.0|60.0| 60.0| 40.0| 40.0| 33.0| 20.0| 47.0|  0.0|  1.0|\n",
      "+----+-----+----+-----+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfpen = ss.createDataFrame(train_rdd, penschema)\n",
    "dfpen.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the data and create an RDD (16 pixels and label)\n",
    "# pen_raw = sc.textFile(\"../Data/penbased.dat\", 4)\n",
    "\n",
    "# pen_raw= pen_raw.map(lambda x: x.split(', ')).map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a DataFrame\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql import Row\n",
    "# penschema = StructType([\n",
    "#     StructField(\"pix1\",DoubleType(),True),\n",
    "#     StructField(\"pix2\",DoubleType(),True),\n",
    "#     StructField(\"pix3\",DoubleType(),True),\n",
    "#     StructField(\"pix4\",DoubleType(),True),\n",
    "#     StructField(\"pix5\",DoubleType(),True),\n",
    "#     StructField(\"pix6\",DoubleType(),True),\n",
    "#     StructField(\"pix7\",DoubleType(),True),\n",
    "#     StructField(\"pix8\",DoubleType(),True),\n",
    "#     StructField(\"pix9\",DoubleType(),True),\n",
    "#     StructField(\"pix10\",DoubleType(),True),\n",
    "#     StructField(\"pix11\",DoubleType(),True),\n",
    "#     StructField(\"pix12\",DoubleType(),True),\n",
    "#     StructField(\"pix13\",DoubleType(),True),\n",
    "#     StructField(\"pix14\",DoubleType(),True),\n",
    "#     StructField(\"pix15\",DoubleType(),True),\n",
    "#     StructField(\"pix16\",DoubleType(),True),\n",
    "#     StructField(\"label\",DoubleType(),True)\n",
    "# ])\n",
    "\n",
    "# dfpen = ss.createDataFrame(pen_raw,penschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| pix1| pix2|pix3| pix4| pix5| pix6| pix7| pix8| pix9|pix10|pix11|pix12|pix13|pix14|pix15|pix16|label|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 47.0|100.0|27.0| 81.0| 57.0| 37.0| 26.0|  0.0|  0.0| 23.0| 56.0| 53.0|100.0| 90.0| 40.0| 98.0|  8.0|\n",
      "|  0.0| 89.0|27.0|100.0| 42.0| 75.0| 29.0| 45.0| 15.0| 15.0| 37.0|  0.0| 69.0|  2.0|100.0|  6.0|  2.0|\n",
      "|  0.0| 57.0|31.0| 68.0| 72.0| 90.0|100.0|100.0| 76.0| 75.0| 50.0| 51.0| 28.0| 25.0| 16.0|  0.0|  1.0|\n",
      "|  0.0|100.0| 7.0| 92.0|  5.0| 68.0| 19.0| 45.0| 86.0| 34.0|100.0| 45.0| 74.0| 23.0| 67.0|  0.0|  4.0|\n",
      "|  0.0| 67.0|49.0| 83.0|100.0|100.0| 81.0| 80.0| 60.0| 60.0| 40.0| 40.0| 33.0| 20.0| 47.0|  0.0|  1.0|\n",
      "|100.0|100.0|88.0| 99.0| 49.0| 74.0| 17.0| 47.0|  0.0| 16.0| 37.0|  0.0| 73.0| 16.0| 20.0| 20.0|  6.0|\n",
      "|  0.0|100.0| 3.0| 72.0| 26.0| 35.0| 85.0| 35.0|100.0| 71.0| 73.0| 97.0| 65.0| 49.0| 66.0|  0.0|  4.0|\n",
      "|  0.0| 39.0| 2.0| 62.0| 11.0|  5.0| 63.0|  0.0|100.0| 43.0| 89.0| 99.0| 36.0|100.0|  0.0| 57.0|  0.0|\n",
      "| 13.0| 89.0|12.0| 50.0| 72.0| 38.0| 56.0|  0.0|  4.0| 17.0|  0.0| 61.0| 32.0| 94.0|100.0|100.0|  5.0|\n",
      "| 74.0| 87.0|31.0|100.0|  0.0| 69.0| 62.0| 64.0|100.0| 79.0|100.0| 38.0| 84.0|  0.0| 18.0|  1.0|  9.0|\n",
      "| 48.0| 96.0|62.0| 65.0| 88.0| 27.0| 21.0|  0.0| 21.0| 33.0| 79.0| 67.0|100.0|100.0|  0.0| 85.0|  8.0|\n",
      "|100.0|100.0|72.0| 99.0| 36.0| 78.0| 34.0| 54.0| 79.0| 47.0| 64.0| 13.0| 19.0|  0.0|  0.0|  2.0|  5.0|\n",
      "| 91.0| 74.0|54.0|100.0|  0.0| 87.0| 23.0| 59.0| 81.0| 67.0|100.0| 39.0| 79.0|  4.0| 21.0|  0.0|  9.0|\n",
      "|  0.0| 85.0|38.0|100.0| 81.0| 88.0| 87.0| 50.0| 84.0| 12.0| 58.0|  0.0| 53.0| 22.0|100.0| 24.0|  7.0|\n",
      "| 35.0| 76.0|57.0|100.0|100.0| 92.0| 68.0| 66.0| 81.0| 38.0| 82.0|  9.0| 32.0|  0.0|  0.0| 17.0|  3.0|\n",
      "| 50.0| 84.0|66.0|100.0| 75.0| 75.0| 51.0| 51.0|100.0| 42.0| 97.0| 13.0| 49.0|  0.0|  0.0|  7.0|  3.0|\n",
      "| 99.0| 80.0|63.0|100.0| 25.0| 76.0| 79.0| 68.0|100.0| 62.0| 97.0| 23.0| 54.0|  0.0|  0.0| 16.0|  9.0|\n",
      "| 24.0| 66.0|43.0|100.0| 59.0| 65.0| 34.0| 28.0|  0.0|  1.0| 16.0| 11.0| 58.0|  0.0|100.0|  1.0|  2.0|\n",
      "|  0.0| 73.0|19.0| 99.0| 72.0|100.0| 70.0| 73.0| 32.0| 48.0|  5.0| 18.0| 46.0|  0.0|100.0|  0.0|  2.0|\n",
      "| 12.0| 77.0|20.0| 62.0| 78.0| 40.0| 50.0|  0.0|  1.0| 17.0|  0.0| 64.0| 23.0| 98.0|100.0|100.0|  5.0|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfpen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data frame includes \"feature\" and \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vector_Assembler(df,y_column):\n",
    "    columns = df.columns\n",
    "    # remove y column\n",
    "    columns.remove(y_column)\n",
    "    va= VectorAssembler(inputCols=columns,outputCol='features').transform(df)\n",
    "    lpoints = va.select(\"features\", y_column).withColumnRenamed(y_column, \"label\")\n",
    "    return lpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 14:38:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[47.0,100.0,27.0,...|  8.0|\n",
      "|[0.0,89.0,27.0,10...|  2.0|\n",
      "|[0.0,57.0,31.0,68...|  1.0|\n",
      "|[0.0,100.0,7.0,92...|  4.0|\n",
      "|[0.0,67.0,49.0,83...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints = Vector_Assembler(dfpen,'label')\n",
    "lpoints.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# va = VectorAssembler(outputCol='features')\n",
    "# va.setInputCols(dfpen.columns[:-1])\n",
    "# va.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into training and vaildation sets.\n",
    "pendtsets = lpoints.randomSplit([.8,.2])\n",
    "train = pendtsets[0].cache()\n",
    "val = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Training and Test data.\n",
    "# pendtsets = penlpoints.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the data.\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(maxBins=32,maxDepth=5,minInstancesPerNode=1,minInfoGain=0)\n",
    "dtmodel = dt.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(16, {0: 0.0238, 1: 0.0649, 2: 0.0038, 3: 0.0882, 4: 0.1004, 7: 0.0024, 8: 0.1029, 9: 0.1181, 10: 0.1055, 11: 0.0046, 12: 0.0304, 13: 0.1288, 14: 0.1084, 15: 0.1179})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtmodel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_3f60c1742fea, depth=5, numNodes=59, numClasses=10, numFeatures=16\n",
      "  If (feature 13 <= 57.5)\n",
      "   If (feature 4 <= 40.5)\n",
      "    If (feature 9 <= 20.5)\n",
      "     If (feature 14 <= 62.5)\n",
      "      If (feature 10 <= 12.5)\n",
      "       Predict: 8.0\n",
      "      Else (feature 10 > 12.5)\n",
      "       Predict: 6.0\n",
      "     Else (feature 14 > 62.5)\n",
      "      If (feature 15 <= 26.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 15 > 26.5)\n",
      "       Predict: 8.0\n",
      "    Else (feature 9 > 20.5)\n",
      "     If (feature 1 <= 99.5)\n",
      "      If (feature 9 <= 62.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 9 > 62.5)\n",
      "       Predict: 9.0\n",
      "     Else (feature 1 > 99.5)\n",
      "      If (feature 13 <= 16.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 13 > 16.5)\n",
      "       Predict: 4.0\n",
      "   Else (feature 4 > 40.5)\n",
      "    If (feature 15 <= 26.5)\n",
      "     If (feature 10 <= 37.5)\n",
      "      If (feature 12 <= 37.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 12 > 37.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 10 > 37.5)\n",
      "      If (feature 3 <= 90.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 90.5)\n",
      "       Predict: 3.0\n",
      "    Else (feature 15 > 26.5)\n",
      "     If (feature 0 <= 32.5)\n",
      "      If (feature 15 <= 63.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 15 > 63.5)\n",
      "       Predict: 8.0\n",
      "     Else (feature 0 > 32.5)\n",
      "      If (feature 15 <= 36.5)\n",
      "       Predict: 6.0\n",
      "      Else (feature 15 > 36.5)\n",
      "       Predict: 8.0\n",
      "  Else (feature 13 > 57.5)\n",
      "   If (feature 8 <= 52.5)\n",
      "    If (feature 14 <= 99.5)\n",
      "     If (feature 12 <= 20.5)\n",
      "      Predict: 5.0\n",
      "     Else (feature 12 > 20.5)\n",
      "      If (feature 11 <= 25.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 11 > 25.5)\n",
      "       Predict: 8.0\n",
      "    Else (feature 14 > 99.5)\n",
      "     If (feature 3 <= 87.5)\n",
      "      If (feature 12 <= 65.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 12 > 65.5)\n",
      "       Predict: 8.0\n",
      "     Else (feature 3 > 87.5)\n",
      "      If (feature 0 <= 35.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 0 > 35.5)\n",
      "       Predict: 8.0\n",
      "   Else (feature 8 > 52.5)\n",
      "    If (feature 10 <= 37.5)\n",
      "     If (feature 3 <= 78.5)\n",
      "      If (feature 0 <= 26.5)\n",
      "       Predict: 9.0\n",
      "      Else (feature 0 > 26.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 78.5)\n",
      "      Predict: 8.0\n",
      "    Else (feature 10 > 37.5)\n",
      "     If (feature 2 <= 49.5)\n",
      "      If (feature 7 <= 42.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 7 > 42.5)\n",
      "       Predict: 9.0\n",
      "     Else (feature 2 > 49.5)\n",
      "      If (feature 4 <= 21.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 4 > 21.5)\n",
      "       Predict: 8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dtmodel._call_java('toDebugString'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,27.0,24....|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,0.0,33.0,12....|  9.0|[6.0,0.0,0.0,0.0,...|[0.375,0.0,0.0,0....|       9.0|\n",
      "|[0.0,0.0,38.0,13....|  9.0|[6.0,0.0,0.0,0.0,...|[0.375,0.0,0.0,0....|       9.0|\n",
      "|[0.0,0.0,44.0,16....|  9.0|[6.0,0.0,0.0,0.0,...|[0.375,0.0,0.0,0....|       9.0|\n",
      "|[0.0,16.0,33.0,43...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,21.0,41.0,46...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,22.0,36.0,47...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,27.0,33.0,50...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,28.0,22.0,49...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,33.0,31.0,60...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,33.0,66.0,60...|  8.0|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|       8.0|\n",
      "|[0.0,34.0,25.0,52...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,34.0,34.0,55...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,34.0,35.0,55...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,34.0,43.0,67...|  1.0|[1.0,162.0,64.0,0...|[0.00396825396825...|       1.0|\n",
      "|[0.0,37.0,34.0,55...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,37.0,39.0,53...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,38.0,32.0,51...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,39.0,35.0,55...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "|[0.0,39.0,47.0,63...|  1.0|[3.0,492.0,0.0,3....|[0.00373134328358...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test data.\n",
    "dtpredicts = dtmodel.transform(val)\n",
    "dtpredicts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "available metrics : https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81892688077048"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expects two input columns: prediction and label.\n",
    "\n",
    "# f1|accuracy(defulat)|weightedPrecision|weightedRecall|weightedTruePositiveRate| \n",
    "# weightedFalsePositiveRate|weightedFMeasure|truePositiveRateByLabel| \n",
    "# falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel| \n",
    "# logLoss|hammingLoss\n",
    "metric_name = \"f1\"\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\\\n",
    "                .setMetricName(metric_name) \n",
    "\n",
    "metrics.evaluate(dtpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-fold validation\n",
    "cross-validation : https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"f1\"\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\\\n",
    "                .setMetricName(metric_name) \n",
    "#ParamGridBuilder() – combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(dtree.maxDepth,[5,10,15,20,25,30])\\\n",
    "            .build()\n",
    "\n",
    "cv = CrossValidator(estimator=dtree,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=5,\n",
    "                   estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/06 16:01:53 WARN CacheManager: Asked to cache already cached data.\n",
      "22/03/06 16:01:53 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "cvmodel =cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Max Depth : 15\n",
      "Accuracy : 0.9592964824120603\n"
     ]
    }
   ],
   "source": [
    "cv_val_pred = cvmodel.bestModel.transform(val)\n",
    "print(\"Best Max Depth : %s\" % cvmodel.bestModel.getMaxDepth())\n",
    "print(\"Accuracy : %s\" % evaluator.evaluate(cv_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (922163746.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [62]\u001b[0;36m\u001b[0m\n\u001b[0;31m    evaluator =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = \n",
    "#ParamGridBuilder() – combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().\n",
    "cv = CrossValidator()\n",
    "\n",
    "cvmodel = cv.\n",
    "print(\"Best Max Depth : %s\" % cvmodel.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Accuracy : %s\" % MulticlassClassificationEvaluator().evaluate(cvmodel.bestModel.transform(pendtvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
