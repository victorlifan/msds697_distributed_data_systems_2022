{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 14:51:45 WARN Utils: Your hostname, Fans-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.1.153.45 instead (on interface en0)\n",
      "22/03/01 14:51:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/01 14:51:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and create an RDD (16 pixels and label)\n",
    "pen_raw = sc.textFile(\"../Data/penbased.dat\", 4)\n",
    "\n",
    "pen_raw= pen_raw.map(lambda x: x.split(', ')).map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "dfpen = ss.createDataFrame(pen_raw,penschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| pix1| pix2|pix3| pix4| pix5| pix6| pix7| pix8| pix9|pix10|pix11|pix12|pix13|pix14|pix15|pix16|label|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 47.0|100.0|27.0| 81.0| 57.0| 37.0| 26.0|  0.0|  0.0| 23.0| 56.0| 53.0|100.0| 90.0| 40.0| 98.0|  8.0|\n",
      "|  0.0| 89.0|27.0|100.0| 42.0| 75.0| 29.0| 45.0| 15.0| 15.0| 37.0|  0.0| 69.0|  2.0|100.0|  6.0|  2.0|\n",
      "|  0.0| 57.0|31.0| 68.0| 72.0| 90.0|100.0|100.0| 76.0| 75.0| 50.0| 51.0| 28.0| 25.0| 16.0|  0.0|  1.0|\n",
      "|  0.0|100.0| 7.0| 92.0|  5.0| 68.0| 19.0| 45.0| 86.0| 34.0|100.0| 45.0| 74.0| 23.0| 67.0|  0.0|  4.0|\n",
      "|  0.0| 67.0|49.0| 83.0|100.0|100.0| 81.0| 80.0| 60.0| 60.0| 40.0| 40.0| 33.0| 20.0| 47.0|  0.0|  1.0|\n",
      "|100.0|100.0|88.0| 99.0| 49.0| 74.0| 17.0| 47.0|  0.0| 16.0| 37.0|  0.0| 73.0| 16.0| 20.0| 20.0|  6.0|\n",
      "|  0.0|100.0| 3.0| 72.0| 26.0| 35.0| 85.0| 35.0|100.0| 71.0| 73.0| 97.0| 65.0| 49.0| 66.0|  0.0|  4.0|\n",
      "|  0.0| 39.0| 2.0| 62.0| 11.0|  5.0| 63.0|  0.0|100.0| 43.0| 89.0| 99.0| 36.0|100.0|  0.0| 57.0|  0.0|\n",
      "| 13.0| 89.0|12.0| 50.0| 72.0| 38.0| 56.0|  0.0|  4.0| 17.0|  0.0| 61.0| 32.0| 94.0|100.0|100.0|  5.0|\n",
      "| 74.0| 87.0|31.0|100.0|  0.0| 69.0| 62.0| 64.0|100.0| 79.0|100.0| 38.0| 84.0|  0.0| 18.0|  1.0|  9.0|\n",
      "| 48.0| 96.0|62.0| 65.0| 88.0| 27.0| 21.0|  0.0| 21.0| 33.0| 79.0| 67.0|100.0|100.0|  0.0| 85.0|  8.0|\n",
      "|100.0|100.0|72.0| 99.0| 36.0| 78.0| 34.0| 54.0| 79.0| 47.0| 64.0| 13.0| 19.0|  0.0|  0.0|  2.0|  5.0|\n",
      "| 91.0| 74.0|54.0|100.0|  0.0| 87.0| 23.0| 59.0| 81.0| 67.0|100.0| 39.0| 79.0|  4.0| 21.0|  0.0|  9.0|\n",
      "|  0.0| 85.0|38.0|100.0| 81.0| 88.0| 87.0| 50.0| 84.0| 12.0| 58.0|  0.0| 53.0| 22.0|100.0| 24.0|  7.0|\n",
      "| 35.0| 76.0|57.0|100.0|100.0| 92.0| 68.0| 66.0| 81.0| 38.0| 82.0|  9.0| 32.0|  0.0|  0.0| 17.0|  3.0|\n",
      "| 50.0| 84.0|66.0|100.0| 75.0| 75.0| 51.0| 51.0|100.0| 42.0| 97.0| 13.0| 49.0|  0.0|  0.0|  7.0|  3.0|\n",
      "| 99.0| 80.0|63.0|100.0| 25.0| 76.0| 79.0| 68.0|100.0| 62.0| 97.0| 23.0| 54.0|  0.0|  0.0| 16.0|  9.0|\n",
      "| 24.0| 66.0|43.0|100.0| 59.0| 65.0| 34.0| 28.0|  0.0|  1.0| 16.0| 11.0| 58.0|  0.0|100.0|  1.0|  2.0|\n",
      "|  0.0| 73.0|19.0| 99.0| 72.0|100.0| 70.0| 73.0| 32.0| 48.0|  5.0| 18.0| 46.0|  0.0|100.0|  0.0|  2.0|\n",
      "| 12.0| 77.0|20.0| 62.0| 78.0| 40.0| 50.0|  0.0|  1.0| 17.0|  0.0| 64.0| 23.0| 98.0|100.0|100.0|  5.0|\n",
      "+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfpen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data frame includes \"feature\" and \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_84f601e78b63"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = VectorAssembler(outputCol='features')\n",
    "va.setInputCols(dfpen.columns[:-1])\n",
    "va.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Test data.\n",
    "pendtsets = penlpoints.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the data.\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtmodel = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtmodel._call_java('toDebugString'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data.\n",
    "dtpredicts = dtmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpredicts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "available metrics : https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "#prediction and label\n",
    "prediction_label = dtpredicts.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "metrics = \n",
    "\n",
    "precision = \n",
    "recall = \n",
    "f1Score = \n",
    "confusionMetrics = \n",
    "\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "print(\"Confusion Metrics = \\n%s\" % confusionMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-fold validation\n",
    "cross-validation : https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = \n",
    "#ParamGridBuilder() â€“ combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().\n",
    "cv = CrossValidator()\n",
    "\n",
    "cvmodel = cv.\n",
    "print(\"Best Max Depth : %s\" % cvmodel.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Accuracy : %s\" % MulticlassClassificationEvaluator().evaluate(cvmodel.bestModel.transform(pendtvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
